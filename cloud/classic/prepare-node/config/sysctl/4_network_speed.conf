#------------------------------------------------------------------------------#
#                     NETWORK SPEED TWEAKS (Khafan Server)                     #
#------------------------------------------------------------------------------#

# Turn on the tcp_timestamps. More accurate timestamp make TCP congestion
# control algorithms work better and are recommended for fast networks.
net.ipv4.tcp_timestamps = 1

# Increase number of incoming connections backlog. Try up to 262144.
net.core.netdev_max_backlog = 16384

# Set max number half open SYN requests to keep in memory.
net.ipv4.tcp_max_syn_backlog = 128
net.ipv4.tcp_max_tw_buckets = 8192

# Do not cache ssthresh from previous connection.
net.ipv4.tcp_no_metrics_save = 1
net.ipv4.tcp_moderate_rcvbuf = 1

# Explicitly set htcp or cubic as the congestion control. To get a list of
# congestion control algorithms that are available in your kernel run:
# command $sysctl net.ipv4.tcp_available_congestion_control and to find algo
# that can be loaded run $grep TCP_CONG /boot/config-$(uname -r).
# also read this - http://fasterdata.es.net/host-tuning/linux/
net.ipv4.tcp_congestion_control = cubic

# For servers with tcp-heavy workloads, enable <fq> queue management scheduler.
# Highly recommended for CentOS7/Debian8 hosts.
# net.core.default_qdisc = pfifo_fast
net.core.default_qdisc = fq

# If you are using Jumbo Frames, also set this tunable.
# net.ipv4.tcp_mtu_probing = 1

# Set max number of queued connections on a socket. The default value usually
# is too low. Raise this value substantially to support bursts of a request.
net.core.somaxconn = 8192

# Try to reuse time-wait connections.
# But don't recycle them (recycle can break clients behind NAT).
net.ipv4.tcp_tw_recycle = 0
net.ipv4.tcp_tw_reuse = 1

# Avoid falling back to slow start after a connection goes idle.
# Keep it 0 usually.
net.ipv4.tcp_slow_start_after_idle = 0

# Enable TCP window scaling for high-throughput blazing fast TCP performance.
net.ipv4.tcp_window_scaling = 1

# Decrease the time default value for tcp_fin_timeout connection.
net.ipv4.tcp_fin_timeout = 15

# Decrease the time default value for connections to keep alive.
net.ipv4.tcp_keepalive_time = 512
net.ipv4.tcp_keepalive_probes = 10
net.ipv4.tcp_keepalive_intvl = 32

# Limit orphans because each orphan can eat up to 16M of unswappable memory.
net.ipv4.tcp_max_orphans = 8192
net.ipv4.tcp_orphan_retries = 0

# Increase the maximum memory used to reassemble IP fragments.
net.ipv4.ipfrag_high_thresh = 262144
net.ipv4.ipfrag_low_thresh = 196608

# Increase size of RPC datagram queue length.
net.unix.max_dgram_qlen = 256

# Do not allow the arp table to become bigger than this
net.ipv4.neigh.default.gc_thresh3 = 1024

# Tell the gc when to become aggressive with arp table cleaning.
net.ipv4.neigh.default.gc_thresh2 = 512

# Adjust where the gc will leave arp table alone.
net.ipv4.neigh.default.gc_thresh1 = 128

# Adjust to arp table gc to clean-up more often
net.ipv4.neigh.default.gc_interval = 30

# Increase TCP queue length in order to reduce a performance spike with
# relation to timestamps generation.
net.ipv4.neigh.default.proxy_qlen = 96
net.ipv4.neigh.default.unres_qlen = 6

# Enable a fix for RFC1337 - time-wait assassination hazards in TCP.
net.ipv4.tcp_rfc1337 = 1

# This will ensure that immediately subsequent connections use the new values.
net.ipv4.route.flush = 1
net.ipv6.route.flush = 1

# Set 1 if you want to disable Path MTU discovery - a technique to determine
# the largest Maximum Transfer Unit possible on your path.
net.ipv4.ip_no_pmtu_disc = 0

# Use Selective ACK which can be used to signify that specific packets are
# missing - therefore helping fast recovery.
net.ipv4.tcp_sack = 1

# Enables Forward Acknowledgment which works with Selective Acknowledgment.
# Intel recommends to turn in always on.
net.ipv4.tcp_fack = 1

# ECN allows end-to-end notification of network congestion without dropping
# packets.
# https://github.com/systemd/systemd/issues/9748
net.ipv4.tcp_ecn = 2
net.ipv4.tcp_ecn_fallback = 1

net.ipv4.tcp_reordering = 3

# Enable(0)/disable(1) the PreQueue entirely. Allow tcp/ip stack prefer
# low latency instead of high throughput. Try option 1 in slow Wi-Fi networks.
# IBM recommend set it to 0.
net.ipv4.tcp_low_latency = 0

# Enables Forward RTO-Recovery (F-RTO) defined in RFC4138. F-RTO is an enhanced
# recovery algorithm for TCP retransmission timeouts. It is usually good to use
# in wireless environments where packet loss is typically due to random radio
# interference rather than intermediate router congestion. Option 1 - basic
# version is enabled. Option 2 enables SACK-enhanced F-RTO if flow uses SACK.
# Default is 2 but changing it to 1 sometimes improves the performance.
net.ipv4.tcp_frto = 1

# 0 - Rate halving based; a smooth and conservative response, results in halved
# cwnd and ssthresh after one RTT. 1 - Very conservative response; not
# recommended because even though being valid, it interacts poorly with the
# rest of Linux TCP, halves cwnd and ssthresh immediately.
# 2 - Aggressive response; undoes congestion control measures that are now
# known to be unnecessary (ignoring the possibility of a lost retransmission
# that would require TCP to be more cautious), cwnd and ssthresh are restored
# to the values prior timeout. Default is 0.
# Also on some distribution this tunable is deprecated.
# net.ipv4.tcp_frto_response = 2

# Enable TCP Fast Open (RFC7413) to send and accept data in the opening
# SYN-packet. It will not do any troubles with not supported hosts
# but it will do guaranteed speedup handshake for supported ones.
net.ipv4.tcp_fastopen = 3

# Set UDP parameters. Adjust them for your network.
net.ipv4.udp_mem = 45792	61058	91584
# net.ipv4.udp_mem = 8388608 12582912 16777216
# net.ipv4.udp_rmem_min = 65536
# net.ipv4.udp_wmem_min = 65536

# Increase Linux auto-tuning TCP buffer limits.
# Set max to 16MB buffer (16777216) for 1GE network.
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.core.rmem_default = 262144
net.core.wmem_default = 262144
net.core.optmem_max = 40960
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216

# Setting 10GE capable host to consume a maximum of 32M-64M per socket ensures
# parallel streams work well and do not consume a majority of system resources.
# Set max 32MB buffer (33554432) for 10GE network.
# net.core.rmem_max = 33554432
# net.core.wmem_max = 33554432
# net.core.rmem_default = 33554432
# net.core.wmem_default = 33554432
# net.core.optmem_max = 40960
# net.ipv4.tcp_rmem = 4096 87380 33554432
# net.ipv4.tcp_wmem = 4096 65536 33554432

# If you have a lot of memory set max 54MB buffer (56623104) for 10GE network.
# net.core.rmem_max = 56623104
# net.core.wmem_max = 56623104
# net.core.rmem_default = 56623104
# net.core.wmem_default = 56623104
# net.core.optmem_max = 40960
# net.ipv4.tcp_rmem = 4096 87380 56623104
# net.ipv4.tcp_wmem = 4096 65536 56623104
